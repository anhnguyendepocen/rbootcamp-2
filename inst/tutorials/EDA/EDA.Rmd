---
title: "Tutorial 1: Exploratory Data Analysis"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

<script type="text/javascript" async
    src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



## Introduction

This module provides an opportunity to put into practice many of the techniques discussed in this short course. The goal is to think through the tool(s) you’ll need and use them properly to answer the questions. This case study continues to use the *Customer* data and, although not comprehensive, provides you the opportunity to perform several steps in the analytic process to include:

1. Organizing your files
2. Importing the data
3. Dealing with missing values
4. Understanding numerical and categorical variables through descriptive statistics 
5. Using visualization to understand your data

For this tutorial, I have preloaded the following packages for your use:

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(readxl)

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, collapse=TRUE)
```

```{r, eval=FALSE}
library(tidyverse)
library(readxl)
```

## Importing the Data

The first step to any data analysis process is to *get* the data. Assuming our customer data is located in an Excel file titled *CustomerData.xlsx* and located in our current working directory, how would you load it?

```{r import-question1, echo=FALSE}
quiz(caption = "Knowledge Check",
question("How would you read in the names of the spreadsheets in the CustomerData.xlsx file?",
  answer("read_excel('CustomerData.xlsx')"),
  answer("excel_sheets('CustomerData.xlsx')", correct = TRUE),
  answer("read_sheets('CustomerData.xlsx')"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  correct = "Correct! For this Excel file there are two spreadsheets:  'Metadata' and 'Data'."
),
question("Assuming you want to import the 'Data' spreadsheet from the CustomerData.xlsx file, what is the correct command?",
  answer("read_excel('CustomerData.xlsx', sheet = 'Data')", correct = TRUE),
  answer("read_excel('CustomerData.xlsx')"),
  answer("excel_sheets('CustomerData.xlsx', sheet = 'Data')"),
  answer("read_csv('CustomerData.xlsx', sheet = 'Data')"),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
)
```

For the following exercises I have pre-loaded the customer data and saved it as a data frame named `customer`.  Let's get a basic feel for this `customer` data set by assessing the following:

1. What are the dimensions of this data set?
2. What are the names of all the variables?
3. What type of variables are in this data (i.e. character, integer, double)?

```{r prepare-data, echo=FALSE}
customer <- rbootcamp::customer
```

```{r understand, exercise=TRUE, exercise.setup = "prepare-data"}

```

```{r understand-hint-1}
dim(customer)
```

```{r understand-hint-2}
names(customer)
```

```{r understand-hint-3}
# you can assess the data with glimpse
glimpse(customer)

# or by looping over each column and assessing the type
sapply(customer, typeof)
```

```{r understand-question1, echo=FALSE}
quiz(caption = "Knowledge Check",
question("What are the dimensions of our customer data?",
  answer("5000 x 60"),
  answer("5000 x 59", correct = TRUE),
  answer("5000"),
  answer("59 x 5000"),
  allow_retry = TRUE,
  random_answer_order = TRUE
),
question("What are the names of the first 3 variables?",
  answer("CustomerID, Region, TownSize", correct = TRUE),
  answer("CustomerID, Gender, Age"),
  answer("Region, Gender, Age"),
  answer("CustomerID, Region Age"),
  allow_retry = TRUE,
  random_answer_order = TRUE
),
question("What type of variable is JobCategory?",
  answer("character", correct = TRUE),
  answer("integer"),
  answer("factor"),
  answer("double"),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
)
```

## Dealing with Missing Data

A common task in data analysis is dealing with missing values.  Let's assess missing values in our `customer` data set.   Go ahead and check out:

1. How many missing values are in the data?
2. Which variables contain these missing values?
3. Omit all rows with missing values in them? Now how many observations does your data have?

```{r missing, exercise=TRUE, exercise.setup = "prepare-data"}

```

```{r missing-hint-1}
sum(is.na(customer))
```

```{r missing-hint-2}
colSums(is.na(customer)) %>%
  sort(decreasing = TRUE)
```

```{r missing-hint-3}
customer <- na.omit(customer)

dim(customer)
```

```{r missing-question1, echo=FALSE}
quiz(caption = "Knowledge Check",
question("How many missing values are in this data set?",
  answer("136"),
  answer("124", correct = TRUE),
  answer("98"),
  answer("32"),
  allow_retry = TRUE,
  random_answer_order = TRUE
),
question("How many variables contain missing values?",
  answer("8", correct = TRUE),
  answer("6"),
  answer("4"),
  answer("2"),
  allow_retry = TRUE,
  random_answer_order = TRUE
),
question("How many missing values are in the Gender variable?",
  answer("33", correct = TRUE),
  answer("16"),
  answer("7"),
  answer("2"),
  allow_retry = TRUE,
  random_answer_order = TRUE
),
question("After you remove all missing values, how many rows are in the clean data set?",
  answer("4893", correct = TRUE),
  answer("4888"),
  answer("4918"),
  answer("4617"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  correct = "That's right!  So by removing missing values we removed 107 customers from our data set."
)
)
```

## Understanding Your Data Through Descriptive Statistics

Descriptive statistics are the first pieces of information used to understand and represent a data set. There goal, in essence, is to describe the main features of numerical and categorical information with simple summaries. Lets perform some descriptive statistics that will help answer the following questions:

### 1. How is average HHIncome distributed across JobCategory? 

The goal is to group our data by *JobCategory* and then compute the mean of the *HHIncome* variable. Rank ordering your data based on the mean income (using `arrange`) will make it easier to interpret the results.

```{r prepare-data2, echo=FALSE}
customer <- na.omit(rbootcamp::customer)
```

```{r descriptive1, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in these functions to answer the question
customer %>%
  group_by() %>% 
  summarize() %>% 
  arrange()
```

```{r descriptive1-hint-1}
# First, we want to group our data by job category
customer %>%
  group_by(JobCategory) %>% 
  summarize() %>% 
  arrange()
```

```{r descriptive1-hint-2}
# Next, we compute our summary statistic
customer %>%
  group_by(JobCategory) %>% 
  summarize(avg_HHIncome = mean(HHIncome)) %>% 
  arrange()
```

```{r descriptive1-solution}
# Finally, we arrange our summary statistic so we can easily see how the
# job categories rank
customer %>%
  group_by(JobCategory) %>% 
  summarize(avg_HHIncome = mean(HHIncome)) %>% 
  arrange(desc(avg_HHIncome))
```

```{r descriptive-question1, echo=FALSE}
quiz(caption = "Knowledge Check",
question("Which JobCategory has the largest average HHIncome?",
  answer("Crafts"),
  answer("Labor", correct = TRUE),
  answer("Sales"),
  answer("Professional"),
  random_answer_order = TRUE
),
question("Which JobCategory has the smallest average HHIncome?",
  answer("Crafts"),
  answer("Labor"),
  answer("Sales", correct = TRUE),
  answer("Professional"),
  random_answer_order = TRUE
)
)
```


### 2. Is there a large difference in the number of customers in each JobCategory?

Count the number of observations for each *JobCategory* level.

```{r descriptive2, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the count function to answer the question
customer %>% 
  count()
```

```{r descriptive2-hint-1}
# We can use count to count the number of observations.
customer %>%
  count(JobCategory)
```


```{r descriptive2-solution}
# We can also include sort = TRUE to sort the output
customer %>%
  count(JobCategory, sort = TRUE)
```

```{r descriptive-question2, echo=FALSE}
quiz(caption = "Knowledge Check",
question("Which JobCategory captures the most customers?",
  answer("Crafts"),
  answer("Sales", correct = TRUE),
  answer("Labor"),
  answer("Professional"),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
question("How many customers are in the *Service* career field?",
  answer("1635"),
  answer("212"),
  answer("610", correct = TRUE),
  answer("452"),
  random_answer_order = TRUE,
  allow_retry = TRUE
)
)
```


### 3. How many customers are in each JobCategory across the regions?

Now count the number of observations for each *JobCategory* and *Region* combination.  Rank ordering your results using `arrange` will make it easier to interpret the results. 

```{r descriptive3, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the count function to answer the question
customer %>%
  count()
```


```{r descriptive3-hint-1}
# We can inspect the number of earners in each category by region with the following:
customer %>%
  count(JobCategory, Region)
```

```{r descriptive3-hint-2}
# by sorting we can get the categories and regions with the most customers
customer %>%
  count(JobCategory, Region, sort = TRUE)
```

```{r descriptive3-hint-3}
# and we can add arrange to identify the categories and regions with the least 
# customers
customer %>%
  count(JobCategory, Region) %>%
  arrange(n)
```

```{r descriptive-question3, echo=FALSE}
quiz(caption = "Knowledge Check",
question("Which JobCategory and Region captures the most customers?",
  answer("Sales in region 4"),
  answer("Sales in region 5", correct = TRUE),
  answer("Agriculture in region 1"),
  answer("Professional in region 5"),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
question("Which JobCategory and Region has the __least__ amount of customers?",
  answer("Sales in region 4"),
  answer("Sales in region 5"),
  answer("Agriculture in region 4", correct = TRUE),
  answer("Professional in region 5"),
  random_answer_order = TRUE,
  allow_retry = TRUE
)
)
```

### 4. How does HHIncome vary across JobCategory, Region and Gender?

The goal is to group our data by *JobCategory*, *Region*, or *Gender* and then compute the standard deviation of the *HHIncome* variable.  You can compute the standard deviation of variable *x* by using the function `sd(x)`. 

```{r descriptive4, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the functions to answer these questions
customer %>%
  group_by() %>% 
  summarize()
```

```{r descriptive4-hint-1}
# first we group by the category of interest (i.e. JobCategory, Region, Gender)
customer %>%
  group_by(JobCategory) %>% 
  summarize()
```

```{r descriptive4-hint-2}
# next we can compute the standard deviation of HHIncome to assess how income
# varies within the category of interest
customer %>%
  group_by(JobCategory) %>% 
  summarize(income_sd = sd(HHIncome))
```

```{r descriptive4-hint-3}
# Last, we rank order the variability
customer %>%
  group_by(JobCategory) %>% 
  summarize(income_sd = sd(HHIncome)) %>%
  arrange(income_sd)
```

```{r descriptive-question4, echo=FALSE}
quiz(caption = "Knowledge Check",
question("Which JobCategory has the __largest__ income variability?",
  answer("Crafts"),
  answer("Sales"),
  answer("Labor", correct = TRUE),
  answer("Professional"),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
question("Which Region has the __smallest__ income variability?",
  answer("4"),
  answer("5"),
  answer("2", correct = TRUE),
  answer("1"),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
question("Is there much difference between Male and Female income variability?",
  answer("No"),
  answer("Yes", correct = TRUE),
  random_answer_order = TRUE,
  correct = "Male incomes have about 20% more variation than females!",
  incorrect = "Male incomes have about 20% more variation than females!"
)
)
```

### 5. What percentage of our customers are married vs. unmarried? 

Compute the percentage of customers that are married vs. unmarried.  Note that the variable of interest here is *MaritalStatus*.  This will require you to count the number of observations in each level of *MaritalStatus* and then use `mutate` to create a new variable that computes the percentage.

```{r descriptive5, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the functions to answer these questions
customer %>%
  count() %>% 
  mutate()
```

```{r descriptive5-hint-1}
# first we count the number of observations that are married vs. unmarried
customer %>%
  count(MaritalStatus) %>% 
  mutate()
```

```{r descriptive5-hint-2}
# next we use mutate to create a new variable that computes the percent of each
customer %>%
  count(MaritalStatus) %>% 
  mutate(pct = n / sum(n))
```


```{r descriptive-question5, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("Are the majority of our customers married or unmarried?",
  answer("Married"),
  answer("Unmarried", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
question("How many customers are married?",
  answer("2346"),
  answer("2599"),
  answer("2348", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
question("What percentage of our customers are married?",
  answer("44%"),
  answer("53%"),
  answer("48%", correct = TRUE),
  answer("56%"),
  random_answer_order = TRUE,
  allow_retry = TRUE
)
)
```

### 6. Are married customers more educated than single customers?

The goal is to group our data by *MaritalStatus* and then compute the average education in years (*EducationYears* variable).  For this exercise, compute the `mean` and `median` education in years for married vs. unmarried customers.

```{r descriptive6, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the functions to answer these questions
customer %>%
  group_by() %>% 
  summarize()
```

```{r descriptive6-hint-1}
# first we group our data based on marital status
customer %>%
  group_by(MaritalStatus) %>% 
  summarize()
```

```{r descriptive6-hint-2}
# next we compute the mean and median education levels
customer %>%
  group_by(MaritalStatus) %>% 
  summarize(med_Ed = mean(EducationYears),
            mean_Ed = mean(EducationYears))
```


```{r descriptive-question6, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("What is the median education level for unmarried customers?",
  answer("13.2 years"),
  answer("14.6 years"),
  answer("14 years"),
  answer("15 years", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE,
  correct = "That's right; I bet you make up 75% of the smarts in your household!"
),
 question("What is the mean education level for married customers?",
  answer("13.9 years"),
  answer("15.1 years"),
  answer("14.1 years"),
  answer("14.5 years", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
question("Are the education levels about the same for married and unmarried?",
  answer("No"),
  answer("Yes", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE,
  correct = "Right, the mean and median education levels are about 14.5 years for both married and unmarried customers."
)
)
```

### 7. Which ages are most likely to lease an automobile?

To answer this question, perform the following sequence:

1. count the number of observations by *Age* and *CarOwnership*
2. create a new variable that computes the percentage of car leasers
3. filter for only those customers that lease
4. arrange your percent leasers variable in descending order

```{r descriptive7, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the functions to answer these questions
customer %>%
  count() %>%
  mutate() %>% 
  filter() %>% 
  arrange()
```

```{r descriptive7-hint-1}
# first we count our data by age and car ownership
customer %>%
  count(Age, CarOwnership) %>%
  mutate() %>% 
  filter() %>% 
  arrange()
```

```{r descriptive7-hint-2}
# next we compute the percentage of leasers versus non leasing activities
# note how I use the round() to round my percents to 2 decimals
customer %>%
  count(Age, CarOwnership) %>%
  mutate(pct_lease = round(n / sum(n), 2)) %>% 
  filter() %>% 
  arrange()
```

```{r descriptive7-hint-3}
# then we can filter for just those customers that lease
customer %>%
  count(Age, CarOwnership) %>%
  mutate(pct_lease = round(n / sum(n), 2)) %>% 
  filter(CarOwnership == "Lease") %>% 
  arrange()
```

```{r descriptive7-solution}
# and lastly we rank order the data based on percent lease
customer %>%
  count(Age, CarOwnership) %>%
  mutate(pct_lease = round(n / sum(n), 2)) %>% 
  filter(CarOwnership == "Lease") %>% 
  arrange(desc(pct_lease))
```


```{r descriptive-question7, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("What age has the highest percentage of leasers?",
  answer("39 years"),
  answer("22 years"),
  answer("65 years"),
  answer("79 years", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
 question("What age has the lowest percentage of leasers?",
  answer("39 years", correct = TRUE),
  answer("22 years"),
  answer("65 years"),
  answer("79 years"),
  random_answer_order = TRUE,
  allow_retry = TRUE
)
)
```

### 8. Which age group tends to have the greatest car value?

The goal is to find the top 5 age groups with the largest average *CarValue*.  You can accomplish this by grouping the data by *Age*, computing the mean *CarValue*, and then using the `top_n()` function to find the top 5.

```{r descriptive8, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the functions to answer these questions
customer %>%
  group_by() %>%
  summarize() %>% 
  top_n(5)
```

```{r descriptive8-hint-1}
# first we group our data by age
customer %>%
  group_by(Age) %>%
  summarize() %>% 
  top_n(5)
```

```{r descriptive8-hint-2}
# next we compute the mean of the CarValue variable
customer %>%
  group_by(Age) %>%
  summarize(avg_value = mean(CarValue)) %>% 
  top_n(5)
```


```{r descriptive8-solution}
customer %>%
  group_by(Age) %>%
  summarize(avg_value = mean(CarValue)) %>% 
  top_n(5)
```


```{r descriptive-question8, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("Which age has the highest average car value?",
  answer("67 years"),
  answer("22 years"),
  answer("57 years"),
  answer("52 years", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE,
  incorrect = "Look closely.  The top_n function keeps the data ordered by age but identifies those ages with the top 5 largest mean car values."
),
 question("What is the average car value for 52 year olds?",
  answer("$37,269", correct = TRUE),
  answer("$32,450"),
  answer("$30,111"),
  answer("$36,986"),
  random_answer_order = TRUE,
  allow_retry = TRUE
)
)
```

### 9. Which credit card has the highest default rate?

The goal is to find the top 5 age groups with the largest average *CarValue*.  You can accomplish this by grouping the data by *Age*, computing the mean *CarValue*, and then using the `top_n()` function to find the top 5.

```{r descriptive9, exercise=TRUE, exercise.setup = "prepare-data2"}
# fill in the functions to answer these questions
customer %>%
  count() %>%
  group_by() %>%
  mutate() %>%
  filter() %>%
  arrange()
```

```{r descriptive9-hint-1}
# first we count the defaults by credit card
customer %>%
  count(CreditCard, LoanDefault)
```

```{r descriptive9-hint-2}
# next we group by credit card and comput the percent of defaulters vs non-defaulters
customer %>%
  count(CreditCard, LoanDefault) %>%
  group_by(CreditCard) %>%
  mutate(pct = n / sum(n))
```


```{r descriptive9-solution}
# last we filter for loan defaulters and arrange in descending order to rank order
customer %>%
  count(CreditCard, LoanDefault) %>%
  group_by(CreditCard) %>%
  mutate(pct = n / sum(n)) %>%
  filter(LoanDefault == "Yes") %>%
  arrange(desc(pct))
```


```{r descriptive-question9, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("Which credit card has the highest loan default?",
  answer("Visa"),
  answer("Disc"),
  answer("AMEX"),
  answer("Othe", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
 question("Which credit card has the second highest loan default?",
  answer("Visa"),
  answer("Disc"),
  answer("AMEX", correct = TRUE),
  answer("Othe"),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
 question("What is the loan default rate for Visa customers?",
  answer("22%"),
  answer("24%"),
  answer("23%", correct = TRUE),
  answer("29%"),
  random_answer_order = TRUE,
  allow_retry = TRUE,
  correct = "What's in your pocket?"
)
)
```

## Understanding Your Data Through Visualization

We can answer the same questions in the previous section by using `ggplot2` to visualize our data.

### 1. How is average HHIncome distributed across JobCategory? 

In the last section we grouped our data by *JobCategory* and then computed the mean of the *HHIncome* variable with the below code.  Can you fill in the `ggplot` code to visualize the results?  You only need to fill in the `x` and `y` variables.  `geom_col()` will plot the required bar chart.

```{r viz1, exercise=TRUE, exercise.setup = "prepare-data2"}
df <- customer %>%
  group_by(JobCategory) %>% 
  summarize(avg_HHIncome = mean(HHIncome))

ggplot(data = df, aes(x =  , y =  )) +
  geom_col()
```

```{r viz1-hint-1}
df <- customer %>%
  group_by(JobCategory) %>% 
  summarize(avg_HHIncome = mean(HHIncome))

ggplot(data = df, aes(x = JobCategory, y =  avg_HHIncome)) +
  geom_col()
```

```{r viz1-hint-2}
# we can even reorder our x-axis based on the income levels
ggplot(data = df, aes(x = reorder(JobCategory, -avg_HHIncome), y =  avg_HHIncome)) +
  geom_col()
```

### 2. Is there a large difference in the number of customers in each JobCategory?

If we just want to count the number of observations for each *JobCategory* level, we can do this easily with a bar chart (`geom_bar`).

Can you fill in the `ggplot` code to visualize the results?  You only need to fill in the `data` and `x` arguments (remember, not all `geom`s require an `x` *and* `y` variable).  `geom_bar()` will take care of the rest.

```{r viz2, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(data = , aes(x =  )) +
  geom_bar()
```

```{r viz2-hint-1}
ggplot(data = customer, aes(x = )) +
  geom_bar()
```

```{r viz2-hint-2}
ggplot(data = customer, aes(x = JobCategory)) +
  geom_bar()
```

### 3. How many customers are in each JobCategory across the regions?

What if we want to look at the number of customers for each *JobCategory* __and__ *Region* combination.  We can do this by *facetting* with `facet_wrap()`.  See if you can complete the ggplot code to do this.  Hint, you just need to add a variable to `facet_wrap()`.

```{r viz3, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(data = customer, aes(x = JobCategory)) +
  geom_bar() +
  facet_wrap()
```

```{r viz3-hint-1}
ggplot(data = customer, aes(x = JobCategory)) +
  geom_bar() +
  facet_wrap(~ Region)
```

### 4. How does HHIncome vary across JobCategory, Region and Gender?

In the last section we answered this question by computing the standard deviation of *HHIncome* across each variable of concern.  Using a histogram (`geom_histogram`) and facetting will help us understand this visually. Complete this code by:

1. Adding the appropriate `x` variable
2. Adding the appropriate `geom_`
3. Adding the `facet_wrap`

```{r viz4, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(data = customer, aes(x = )) +
  
  
```

```{r viz4-hint-1}
# assess how HHIncome is distributed across JobCategory
ggplot(data = customer, aes(x = HHIncome)) +
  geom_histogram() +
  facet_wrap(~ JobCategory)
```

```{r viz4-hint-2}
# assess how HHIncome is distributed across JobCategory
ggplot(data = customer, aes(x = HHIncome)) +
  geom_histogram() +
  facet_wrap(~ Region)
```

```{r viz4-hint-3}
# assess how HHIncome is distributed across JobCategory
ggplot(data = customer, aes(x = HHIncome)) +
  geom_histogram() +
  facet_wrap(~ Gender)
```

Given that income is so skewed, see if you can adjust the x-axis scale to be on a logarithmic scale.  Hint, you only need to add a `scale_x_` function to the following code.

```{r viz4a, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(data = customer, aes(x = HHIncome)) +
  geom_histogram() +
  facet_wrap(~ Gender)
```

```{r viz4a-hint-1}
ggplot(data = customer, aes(x = HHIncome)) +
  geom_histogram() +
  facet_wrap(~ Gender) +
  scale_x_log10()
```

### 5. What percentage of our customers are married vs. unmarried? 

In the last section we computed the percentage of married vs. unmarried customers with the following code.  Can you pipe (`%>%`) the results of this code directly into `ggplot` and plot a bar chart to illustrate the results?

```{r viz5, exercise=TRUE, exercise.setup = "prepare-data2"}
customer %>%
  count(MaritalStatus) %>% 
  mutate(pct = n / sum(n))
```

```{r viz5-hint-1}
# first you need to pipe it into ggplot and now you just need to add the 
# appropriate geom (hint: geom_col)
customer %>%
  count(MaritalStatus) %>% 
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = MaritalStatus, y = pct))
```

```{r viz5-hint-2}
customer %>%
  count(MaritalStatus) %>% 
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = MaritalStatus, y = pct)) +
  geom_col()
```

That's right!  Now see how the final plot had its y-axis in decimal form even though they represent percentages.  See if you can add the right arguments in the `scale_y_continuous()` function below to change this axis to be formatted as percentages.

```{r viz5a, exercise=TRUE, exercise.setup = "prepare-data2"}
customer %>%
  count(MaritalStatus) %>% 
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = MaritalStatus, y = pct)) +
  geom_col() +
  scale_y_continuous()
```

```{r viz5a-solution}
customer %>%
  count(MaritalStatus) %>% 
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = MaritalStatus, y = pct)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent)
```

### 6. Are married customers more educated than single customers?

To answer this questions, in the last section we simply computed the mean and median.  Here, let's compare the distribution with a density plot.  In this case, we want to create a density plot of the *EducationYears* variable but overlay the distribution of married vs. unmarried.  See if you can fill in the arguments for:

- `x` which is the variable we want to see the distribution of
- `fill` which will color the distributions based on marital status
- `alpha` which accepts values 0-1 and controls the opacity

```{r viz6, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(customer, aes(x = , fill = )) +
  geom_density(alpha = )
```

```{r viz6-hint-1}
# We want to see the distribution of education years
ggplot(customer, aes(x = EducationYears, fill = )) +
  geom_density(alpha = )
```

```{r viz6-hint-2}
# we use fill to compare the distributions across the marital status categories
ggplot(customer, aes(x = EducationYears, fill = MaritalStatus)) +
  geom_density(alpha = )
```

```{r viz6-hint-3}
# and we use alpha to make the distributions transparent which allows us to see
# how they overlap
ggplot(customer, aes(x = EducationYears, fill = MaritalStatus)) +
  geom_density(alpha = .5)
```


### 7. How do monthly expenditures relate to debt?

Now we'll detour from some of the previously asked questions and inject a new one.  Let's assess how credit card debt (*CreditDebt*) and income (*HHIncome*) are related.  First, fill in the code below to assess the relationships between these two variables across all customers.

```{r viz7, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(customer, aes(x = , y = )) +
  geom_point()
```

```{r viz7-solution}
ggplot(customer, aes(x = HHIncome, y = CreditDebt)) +
  geom_point()
```

###

This isn't a very revealing plot; however, financial data is often heavily skewed.  Can you adjust the x and y axes to be on logarithmic scale?

```{r viz7b, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(customer, aes(x = HHIncome, y = CreditDebt)) +
  geom_point()
```

```{r viz7b-solution}
ggplot(customer, aes(x = HHIncome, y = CreditDebt)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()
```

###

We see that when we transform our axes, a stronger relationship emerges.  Let's add a linear relationship line with `geom_smooth` to illustrate the relationship.

```{r viz7c, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(customer, aes(x = HHIncome, y = CreditDebt)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()
```

```{r viz7c-solution}
ggplot(customer, aes(x = HHIncome, y = CreditDebt)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method = "lm")
```

###

Let's expand on this and assess if those who have defaulted on loans tend to have a higher credit debt.  Naturally, we would assume yes but let's see what the data says.  Go ahead and build onto the below code to allow you to compare this relationship across loan defaulters (*LoanDefault*).  You could do this by coloring, facetting, etc.

```{r viz7d, exercise=TRUE, exercise.setup = "prepare-data2"}
ggplot(customer, aes(x = HHIncome, y = CreditDebt)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method = "lm")
```

```{r viz7d-hint-1}
# this option facets by LoanDefault
ggplot(customer, aes(x = HHIncome, y = CreditDebt)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method = "lm") +
  facet_wrap(~ LoanDefault)
```

```{r viz7d-hint-2}
# this option colors by LoanDefault and allows us to see the difference in the
# relationship between defaulters and non-defaulters
ggplot(customer, aes(x = HHIncome, y = CreditDebt, color = LoanDefault)) +
  geom_point(alpha = .10) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method = "lm")
```

###

At this point, a couple questions may be raised:

1. Is there a difference in the credit debt between loan defaulters and non-defaulters?
2. How should we interpret this relationship between credit debt and income?

We'll answer these in the next two sections.

## Comparing Groups

Hypothesis testing is a useful statistical tool that can be used to draw a conclusion about the population from a sample. In the last section we ended by asking the question *"is there a difference in the credit debt between loan defaulters and non-defaulters?"*  This question can be answered with fundamental hypothesis testing.

### One-sample t-test

The one-sample t-test compares a sample’s mean with a known value, when the variance of the population is <u>unknown</u>. Consider we want to assess the credit debt of our customers and compare it to a certain value. For example, let’s assume the nation-wide average of credit debt is 1.5 ($1.50 of debt for every $1.00 of income) and we want to know if the mean is significantly different than the national average.

To test if our customer's average debt is different than the national average we can use the `t.test` function. The results below show that our mean is 1.87 (which is greater than the average of 1.5). Furthermore, the p-value < 0.05 and 95% confidence interval suggests that our customer's credit debt levels are, on average, statistically different than the national average.

```{r, echo=FALSE}
customer <- na.omit(rbootcamp::customer)
```


```{r}
t.test(customer$CreditDebt, mu = 1.5)
```

We can also modify our `t.test` if we only want to test in one direction.  For example, since our customers' average credit debt is 1.87 we could test if this is statistically __*greater*__ than the national average of 1.5 by including `alternative = "greater"` (note that the default is "two.sided" and there is also a "less" option).

```{r}
t.test(customer$CreditDebt, mu = 1.5, alternative = "greater")
```

However, keep in mind that the `t.test` assumes our data is normally distributed.  We can test if this data is normally distributed with `shapiro.test`.  Since the below results have a p-value < 0.05, this variable is *not* normally distributed.

```{r}
shapiro.test(customer$CreditDebt)
```

We can easily see this by looking at the histogram:

```{r}
p1 <- ggplot(customer, aes(x = CreditDebt)) +
  geom_histogram() +
  ggtitle("Credit Debt Distribution")

p2 <- ggplot(customer, aes(x = CreditDebt)) +
  geom_histogram() +
  scale_x_log10() +
  ggtitle("Log-transformed Distribution")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

Since our data is not normally distributed, we can use the nonparametric Wilcoxon signed-rank test with `wilcox.test` to provide us a more accurate result.  Below, we see that our p-value = 1, which suggests that our average customer debt *is not* statistically different than the national average!

```{r}
wilcox.test(customer$CreditDebt, mu = 1.5, alternative = "greater")
```

###

The last I checked the national average income is about $51,000.  Compare our customer's income to the national average.

```{r compare, exercise=TRUE, exercise.setup = "prepare-data2"}

```

```{r compare-hint-1}
# check the mean of our customer's income
mean(customer$HHIncome)
```

```{r compare-hint-2}
# check whether this variable is normally distributed
shapiro.test(customer$HHIncome)

# or visually
ggplot(customer, aes(x = HHIncome)) +
  geom_histogram()
```

```{r compare-hint-3}
# use wilcox.test since data is not normally distributed
wilcox.test(customer$HHIncome, mu = 51000)

# if you use "greater" and "less" you'll see that our customer data is less than
# the national average
wilcox.test(customer$HHIncome, mu = 51000, alternative = "less")

# view this visually
ggplot(customer, aes(HHIncome)) +
  geom_histogram() +
  geom_vline(xintercept = 51000) +
  scale_x_log10(labels = scales::dollar)
```

```{r compare-question, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("What is the mean income of our customers?",
  answer("$49,010"),
  answer("$62,316"),
  answer("$52,148"),
  answer("$55,002", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
 question("Is HHIncome normally distributed?",
  answer("Yes"),
  answer("No", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
 question("Is there a statistical difference between our customers average income and the national average income?",
  answer("No"),
  answer("Yes", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE,
  incorrect = "Be sure to test the greater than and less than alternatives.",
  correct = "That's right, when we account for the skewness of our data we find that our customers typically have incomes below the national average."
)
)
```


### Two-sample t-test

Now let’s say we want to compare the differences between the average credit debt levels for those customers that have defaulted on a loan vs those that have not. Here, we want to perform a two-sample t-test to see if they are statistically different.  

In this case we can modify our `t.test`.  Here we are comparing the credit debt levels between the two categories of loan defaulters ("Yes" vs. "No") with `CreditDebt ~ LoanDefault`.  The results state that the average credit debt for non-defaulters is 1.44 and the average credit debt for defaulters is 3.26.  Plus, we see that the results are statistically significant (`p-value < 2.2e-16`).

```{r}
t.test(CreditDebt ~ LoanDefault, data = customer)
```

###

However, keep in mind that our `CreditDebt` variable is not normally distributed. Go ahead and test it with `wilcox.test`:

```{r compare2, exercise=TRUE, exercise.setup = "prepare-data2"}

```

```{r compare2-solution}
wilcox.test(CreditDebt ~ LoanDefault, data = customer)
```

```{r compare2-question, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("Is there a statistical difference in credit debt levels between loan defaulters and non-defaulters?",
  answer("No"),
  answer("Yes", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE,
  correct = "Correct!  The p-value for the wilcox.test is less than 0.05",
  incorrect = "Check again. The p-value for the wilcox.test is less than 0.05"
)
)
```

###

Suppose we wish to extend this test.  Rather than just test for differences between two groups, what if we wish to test whether the mean value is the same across multiple categories.  For example, what if we desire to test if the mean credit debt is equal across all regions or job categories. Here we turn to analysis of variance (ANOVA).

### Analysis of Variance

ANOVA will test the hypotheses:

- $H_0: \mu_{group_1} = \mu_{group_2} = \cdots = \mu_{group_n}$

- $H_a: \text{Not all the population means are equal}$

To perform ANOVA in R we can use the `aov` function. The summary of our `anova` object shows us several statistics.  The primary one of interest is the `Pr(>F)`, which is our p-value.  Here our p-value < 0.05, which suggests that not all job categories have the same credit debt levels, on average.  It basically says that at least one of the job categories' credit debt differs from another job category; however, this doesn't tell us which one.

```{r}
anova <- aov(CreditDebt ~ JobCategory, data = customer)
summary(anova)
```

###

To identify which job categories differ with regard to their credit debt levels we can use the `TukeyHSD` function. The output of `TukeyHSD` provides the pairwise differences (`diff`) for the cut categories, the 95% confidence range (`lwr`, `upr`) and the p-value (`p adj`) for that difference.

If you look for the rows with the small p-values you will find the job categories that have significant differences:

- The credit debt levels for the Sales job category are 0.80 *lower* than the Craft category
- Sales is 1.01 *lower* than Labor
- Sales is 0.62 *lower* than Professional
- Sales is 0.93 *lower* than Service

```{r}
TukeyHSD(anova)
```

To make it easier to identify differences, we can easily view this information with `plot`.  This is plotting the pair-wise comparisons in the same order as the table output above.

```{r}
diff <- TukeyHSD(anova)

par(mar = c(5, 10, 3, 1)) # this just modifies the plot margins
plot(diff, las = 1)
```

Basically, customers in the Sales job category tend to have lower credit debt than most other job categories.

###

Go ahead and compare the credit debt levels across the different regions.  Are there any regions that have statistical difference in credit debt levels?

```{r compare3, exercise=TRUE, exercise.setup = "prepare-data2"}

```

```{r compare3-solution}
anova <- aov(CreditDebt ~ Region, data = customer)
summary(anova)
```

```{r compare3-question, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("Are there any regions that have statistical difference in credit debt levels?",
  answer("Yes"),
  answer("No", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE,
  correct = "Correct!  The p-value for the ANOVA test is is not significant.",
  incorrect = "Check again. The p-value for the ANOVA test is is not significant."
)
)
```


## Understanding Relationships

A big part of EDA is to try and understand relationships between variables.  One of the most common ways to identify relationships is through *correlation* analysis.  Correlation is a bivariate analysis that measures the extent that two variables are related (“co-related”) to one another. The value of the correlation coefficient varies between +1 and -1. When the value of the correlation coefficient lies around ±1, then it is said to be a perfect degree of association between the two variables (near +1 implies a strong positive association and near -1 implies a strong negative association). As the correlation coefficient nears 0, the relationship between the two variables weakens with a near 0 value implying no association between the two variables. 

###

As we saw earlier, there appears to be a relationship between income and credit debt. Furthermore, there appears to be a difference in this relationship for loan defaulters and non-defaulters.

```{r}
ggplot(customer, aes(x = HHIncome, y = CreditDebt, color = LoanDefault)) +
  geom_point(alpha = .10) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method = "lm")
```

###

R provides multiple functions to analyze correlations.  To calculate the correlation between two variables we use `cor()`.  When using `cor()` there are two arguments (other than the variables) that need to be considered.  

1. The first is `use =` which allows us to decide how to handle missing data. The default is `use = everything` but if there is missing data in your data set this will cause the output to be `NA` unless we explicitly state to only use complete observations with `use = complete.obs`. Since we've removed missing values this should not be a problem.
2. The second argument is `method =` which allows us to specify if we want to use "pearson", "kendall", or "spearman". Pearson is the default method.

Here we see that the correlation between income and credit debt is 0.67 when using the Pearson's method; however, the correlation is 0.58 when using the Spearman method.  In this case, since our variables are not normally distributed the Spearman method is provided a more accurate correlation.

```{r, message=FALSE, warning=FALSE}
cor(customer$HHIncome, customer$CreditDebt)
cor(customer$HHIncome, customer$CreditDebt, method = "spearman")
```

###

Unfortunately `cor()` only provides the *r* coefficient(s) and does not test for significance nor provide confidence intervals.  To get these parameters for a simple two variable analysis I use `cor.test()`.  In our example we see that the *p*-value is significant.  When you use the Peason method the results will provide the 95% confidence interval; however, the Spearman method does not.

```{r, message=FALSE, warning=FALSE}
cor.test(customer$HHIncome, customer$CreditDebt, method = "spearman")
```

###

Go ahead and compute the correlation between income (*HHIncome*) and month credit card expenditures (*CardSpendMonth*).

```{r corr, exercise=TRUE, exercise.setup = "prepare-data2", message=FALSE, warning=FALSE}

```

```{r corr-solution}
# Pearson method
cor.test(customer$HHIncome, customer$CardSpendMonth)

# Spearman method
cor.test(customer$HHIncome, customer$CardSpendMonth, method = "spearman")
```

```{r corr-question, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("What is the correlation coefficient using the Pearson method?",
  answer("0.612"),
  answer("0.425"),
  answer("0.348", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
 question("Is this correlation significant?",
  answer("No"),
  answer("Yes", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
)
)
```

###

You can also incorporate this into piped functions to streamline your analyses.  For instance, you can calculate the correlation (and p-value) within `summarize`.

```{r, message=FALSE, warning=FALSE}
customer %>%
  summarize(r = cor(HHIncome, CreditDebt, method = "spearman"),
            p_value = cor.test(HHIncome, CreditDebt, method = "spearman")$p.value)
```

Why is this important?  Because you may want to understand relationships at different categorical levels.  For instance, does the correlation between income and credit debt differ depending on the whether or not the customers have defaulted on a loan or not? To answer this we just incorporate a `group_by` statement to calculate the correlations and associated p-values for defaulters vs. non-defaulters.  

Here, we see that the correlation for customers that *have* previously defaulted is 0.72 whereas the correlation for customers that *have not* previously defaulted is 0.57.  Thus, customers with a previous default tend to have higher credit debt to income ratios (probably explains why they've previously defaulted!.

```{r, message=FALSE, warning=FALSE}
customer %>%
  group_by(LoanDefault) %>%
  summarize(r = cor(HHIncome, CreditDebt, method = "spearman"),
            p_value = cor.test(HHIncome, CreditDebt, method = "spearman")$p.value)
```

###

Go ahead and compute the correlation between income and credit debt for each job category.  

```{r corr2, exercise=TRUE, exercise.setup = "prepare-data2", message=FALSE, warning=FALSE}

```

```{r corr2-hint-1}
# first group the data by region.  now just fill in the summarize function to
# compute the correlatino and extract the p-values
customer %>%
  group_by(Region) %>%
  summarize(r = ),
            p_value = )
```

```{r corr2-solution}
customer %>%
  group_by(Region) %>%
  summarize(r = cor(HHIncome, CreditDebt, method = "spearman"),
            p_value = cor.test(HHIncome, CreditDebt, method = "spearman")$p.value)
```

```{r corr2-question, echo=FALSE}
quiz(caption = "Knowledge Check",
 question("What is the correlation coefficient for region 2?",
  answer("0.59"),
  answer("0.52"),
  answer("0.61", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
),
 question("What is the correlation coefficient for region 4?",
  answer("0.59"),
  answer("0.61"),
  answer("0.54", correct = TRUE),
  random_answer_order = TRUE,
  allow_retry = TRUE
)
)
```

###

Although correlation provides a measurement of the relationship strength between two variables, it does not provide us with much more information. To gain more insights we can use linear regression.  However, this is going beyond the intent of this course.

## Continue learning

There are many tutorials at [http://uc-r.github.io/](http://uc-r.github.io/) and I continue to add to them each month.  This is a great place to start building your fundamental R skills.

Also, if you enjoyed learning in this interactive environment and would like to continue to learn about some fundamental analytic techniques such as:

- Unsupervised learning with principal components analysis & cluster analysis
- Supervised predictions with linear regression
- Supervised classification with logistic regression & discriminant analysis
- Resampling
- Linear regression model selection

Then close out of this tutorial and run the following code:

```{r, eval=FALSE}
# install package
devtools::install_github("bradleyboehmke/learningAnalytics")

# load package
library(learningAnalytics)
```

You can access 7 (1 intro and 6 analytic) tutorials:

1. "Hello": An introduction
2. "EDA": Exploratory Data Analysis
3. "Unsupervised": Principal Components Analysis & Cluster Analysis
4. "Linear Regression": Linear Regression
5. "Supervised Classification": Logistic Regression & Discriminant Analysis
6. "Resampling": Leave-One-Out Cross-Validation, *k*-Fold Cross Validation, & Bootstrapping
7. "Model Selection": Best Subset & Stepwise Selection for Linear Models

Access any one of these using code such as:

```{r, eval=FALSE}
get_tutorial("Linear Regression")
```

__*ENJOY!*__




